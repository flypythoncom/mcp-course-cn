# MCP的架构组件

在上一节中，我们讨论了MCP的关键概念和术语。现在，让我们深入探讨构成MCP生态系统的架构组件。

## 主机、客户端和服务器

模型上下文协议(MCP)建立在一个客户端-服务器架构上，该架构使AI模型和外部系统之间能够进行结构化通信。

![MCP架构](https://huggingface.co/datasets/mcp-course/images/resolve/main/unit1/4.png)

MCP架构由三个主要组件组成，每个组件都有明确定义的角色和责任：主机、客户端和服务器。我们在上一节中简单提到了这些，但现在让我们深入探讨每个组件及其责任。

### 主机

**主机**是终端用户直接交互的面向用户的AI应用程序。

例子包括：
- AI聊天应用，如OpenAI ChatGPT或Anthropic的Claude Desktop
- AI增强的IDE，如Cursor，或与Continue.dev等工具的集成
- 在LangChain或smolagents等库中构建的自定义AI代理和应用程序

主机的责任包括：
- 管理用户交互和权限
- 通过MCP客户端启动与MCP服务器的连接
- 编排用户请求、LLM处理和外部工具之间的整体流程
- 以连贯的格式向用户呈现结果

在大多数情况下，用户将根据自己的需求和偏好选择主机应用程序。例如，开发人员可能会选择Cursor的强大代码编辑功能，而领域专家可能会使用在smolagents中构建的自定义应用程序。

### 客户端

**客户端**是主机应用程序中的一个组件，用于管理与特定MCP服务器的通信。关键特性包括：

- 每个客户端与单个服务器维持1:1的连接
- 处理MCP通信的协议级细节
- 作为主机逻辑和外部服务器之间的中介

### 服务器

**服务器**是一个通过MCP协议向AI模型暴露能力的外部程序或服务。服务器：

- 提供对特定外部工具、数据源或服务的访问
- 作为现有功能的轻量级包装器
- 可以在本地（与主机在同一台机器上）或远程（通过网络）运行
- 以客户端可以发现和使用的标准格式暴露其能力

## 通信流程

让我们来看看这些组件在典型的MCP工作流中如何交互：

<Tip>

在下一节中，我们将通过实际示例深入探讨支持这些组件的通信协议。

</Tip>

1. **用户交互**：用户与**主机**应用程序交互，表达意图或查询。

2. **主机处理**：**主机**处理用户的输入，可能使用LLM来理解请求并确定可能需要哪些外部能力。

3. **客户端连接**：**主机**指示其**客户端**组件连接到适当的服务器。

4. **能力发现**：**客户端**查询**服务器**以发现它提供的能力（工具、资源、提示）。

5. **能力调用**：基于用户的需求或LLM的决定，主机指示**客户端**调用**服务器**的特定能力。

6. **服务器执行**：**服务器**执行请求的功能并将结果返回给**客户端**。

7. **结果集成**：**客户端**将这些结果传回**主机**，主机将它们整合到LLM的上下文中或直接呈现给用户。

这种架构的一个关键优势是其模块化。一个**主机**可以通过不同的**客户端**同时连接到多个**服务器**。新的**服务器**可以添加到生态系统中，而不需要更改现有的**主机**。不同**服务器**的能力可以轻松组合。

<Tip>

正如我们在上一节中讨论的，这种模块化将传统的M×N集成问题（M个AI应用程序连接到N个工具/服务）转变为更易管理的M+N问题，其中每个主机和服务器只需实现MCP标准一次。

</Tip>

这个架构看起来可能很简单，但其力量在于通信协议的标准化和组件之间责任的明确分离。这种设计允许形成一个凝聚力强的生态系统，其中AI模型可以与不断增长的外部工具和数据源无缝连接。

## 结论

这些交互模式由几个关键原则指导，这些原则塑造了MCP的设计和演进。该协议通过为AI连接提供通用协议强调**标准化**，同时通过保持核心协议简单但支持高级功能来维持**简单性**。**安全性**是优先考虑的，需要对敏感操作进行明确的用户批准，而可发现性则支持能力的动态发现。该协议的构建考虑到了**可扩展性**，通过版本控制和能力协商支持演进，并确保跨不同实现和环境的**互操作性**。

在下一节中，我们将探索使这些组件能够有效协同工作的通信协议。 