# 理解MCP能力

MCP服务器通过通信协议向客户端暴露各种能力。这些能力分为四个主要类别，每个类别具有不同的特征和用例。让我们探索这些构成MCP功能基础的核心原语。

<Tip>

在本节中，我们将以各种语言中的框架无关函数展示示例。这是为了专注于概念以及它们如何协同工作，而不是任何框架的复杂性。

在接下来的单元中，我们将展示这些概念如何在MCP特定代码中实现。

</Tip>

## 工具

工具是AI模型可以通过MCP协议调用的可执行函数或操作。

- **控制**：工具通常由**模型控制**，这意味着AI模型（LLM）根据用户的请求和上下文决定何时调用它们。
- **安全性**：由于它们能够执行有副作用的操作，工具执行可能会有危险。因此，它们通常需要明确的用户批准。
- **用例**：发送消息、创建工单、查询API、执行计算。

**示例**：一个获取给定位置当前天气数据的天气工具：

<hfoptions id="tool-example">
<hfoption id="python">

```python
def get_weather(location: str) -> dict:
    """获取指定位置的当前天气。"""
    # 连接到天气API并获取数据
    return {
        "temperature": 72,
        "conditions": "Sunny",
        "humidity": 45
    }
```

</hfoption>
<hfoption id="javascript">

```javascript
function getWeather(location) {
    // 连接到天气API并获取数据
    return {
        temperature: 72,
        conditions: 'Sunny',
        humidity: 45
    };
}
```

</hfoption>
</hfoptions>

## 资源

资源提供对数据源的只读访问，允许AI模型检索上下文而不执行复杂逻辑。

- **控制**：资源是**应用程序控制的**，这意味着主机应用程序通常决定何时访问它们。
- **性质**：它们设计用于数据检索，计算量最小，类似于REST API中的GET端点。
- **安全性**：由于它们是只读的，它们通常比工具呈现较低的安全风险。
- **用例**：访问文件内容、检索数据库记录、读取配置信息。

**示例**：提供对文件内容访问的资源：

<hfoptions id="resource-example">
<hfoption id="python">

```python
def read_file(file_path: str) -> str:
    """读取指定路径的文件内容。"""
    with open(file_path, 'r') as f:
        return f.read()
```

</hfoption>
<hfoption id="javascript">

```javascript
function readFile(filePath) {
    // 使用fs.readFile读取文件内容
    const fs = require('fs');
    return new Promise((resolve, reject) => {
        fs.readFile(filePath, 'utf8', (err, data) => {
            if (err) {
                reject(err);
                return;
            }
            resolve(data);
        });
    });
}
```

</hfoption>
</hfoptions>

## 提示

提示是预定义的模板或工作流，用于指导用户、AI模型和服务器能力之间的交互。

- **控制**：提示是**用户控制的**，通常作为主机应用程序UI中的选项呈现。
- **目的**：它们构建交互以最佳地使用可用的工具和资源。
- **选择**：用户通常在AI模型开始处理之前选择提示，为交互设置上下文。
- **用例**：常见工作流、专门的任务模板、引导式交互。

**示例**：用于生成代码审查的提示模板：

<hfoptions id="prompt-example">
<hfoption id="python">

```python
def code_review(code: str, language: str) -> list:
    """为提供的代码片段生成代码审查。"""
    return [
        {
            "role": "system",
            "content": f"您是一名审查{language}代码的代码审查员。提供详细的审查，突出最佳实践、潜在问题和改进建议。"
        },
        {
            "role": "user",
            "content": f"请审查这段{language}代码：\n\n```{language}\n{code}\n```"
        }
    ]
```

</hfoption>
<hfoption id="javascript">

```javascript
function codeReview(code, language) {
    return [
        {
            role: 'system',
            content: `您是一名审查${language}代码的代码审查员。提供详细的审查，突出最佳实践、潜在问题和改进建议。`
        },
        {
            role: 'user',
            content: `请审查这段${language}代码：\n\n\`\`\`${language}\n${code}\n\`\`\``
        }
    ];
}
```

</hfoption>
</hfoptions>

## 采样

采样允许服务器请求客户端（特别是主机应用程序）执行LLM交互。

- **控制**：采样是**服务器启动的**，但需要客户端/主机协助。
- **目的**：它支持服务器驱动的代理行为，以及潜在的递归或多步骤交互。
- **安全性**：像工具一样，采样操作通常需要用户批准。
- **用例**：复杂的多步骤任务、自主代理工作流、交互式过程。

**示例**：服务器可能请求客户端分析它处理过的数据：

<hfoptions id="sampling-example">
<hfoption id="python">

```python
def request_sampling(messages, system_prompt=None, include_context="none"):
    """请求客户端进行LLM采样。"""
    # 在实际实现中，这会向客户端发送请求
    return {
        "role": "assistant",
        "content": "对提供数据的分析..."
    }
```

</hfoption>
<hfoption id="javascript">

```javascript
function requestSampling(messages, systemPrompt = null, includeContext = 'none') {
    // 在实际实现中，这会向客户端发送请求
    return {
        role: 'assistant',
        content: '对提供数据的分析...'
    };
}

function handleSamplingRequest(request) {
    const { messages, systemPrompt, includeContext } = request;
    // 在实际实现中，这会处理请求并返回响应
    return {
        role: 'assistant',
        content: '对采样请求的响应...'
    };
}
```

</hfoption>
</hfoptions>

采样流程遵循以下步骤：
1. 服务器向客户端发送`sampling/createMessage`请求
2. 客户端审查请求并可以修改它
3. 客户端从LLM采样
4. 客户端审查完成结果
5. 客户端将结果返回给服务器

<Tip>

这种人在环路设计确保用户保持对LLM看到和生成的内容的控制。在实现采样时，提供清晰、结构良好的提示并包含相关上下文是很重要的。

</Tip>

## 能力如何协同工作

让我们看看这些能力如何协同工作，实现复杂的交互。在下表中，我们概述了能力、控制者、控制方向以及一些其他细节。

| 能力 | 控制者 | 方向 | 副作用 | 需要批准 | 典型用例 |
|------------|---------------|-----------|--------------|-----------------|-------------------|
| 工具      | 模型 (LLM)   | 客户端 → 服务器 | 是（潜在的） | 是 | 操作、API调用、数据操作 |
| 资源  | 应用程序   | 客户端 → 服务器 | 否（只读） | 通常不 | 数据检索、上下文收集 |
| 提示    | 用户          | 服务器 → 客户端 | 否 | 否（由用户选择） | 引导式工作流、专门模板 |
| 采样   | 服务器        | 服务器 → 客户端 → 服务器 | 间接 | 是 | 多步骤任务、代理行为 |

这些能力设计为以互补的方式协同工作：

1. 用户可能选择一个**提示**来启动专门的工作流
2. 提示可能包括来自**资源**的上下文
3. 在处理过程中，AI模型可能调用**工具**来执行特定操作
4. 对于复杂操作，服务器可能使用**采样**来请求额外的LLM处理

这些原语之间的区别为MCP交互提供了清晰的结构，使AI模型能够访问信息、执行操作并参与复杂的工作流，同时保持适当的控制边界。

## 发现过程

MCP的一个关键特性是动态能力发现。当客户端连接到服务器时，它可以通过特定的列表方法查询可用的工具、资源和提示：

- `tools/list`：发现可用的工具
- `resources/list`：发现可用的资源
- `prompts/list`：发现可用的提示

这种动态发现机制允许客户端适应每个服务器提供的特定能力，而不需要硬编码服务器功能的知识。

## 结论

理解这些核心原语对于有效地使用MCP至关重要。通过提供具有清晰控制边界的不同类型的能力，MCP在维持适当的安全性和控制机制的同时，实现了AI模型与外部系统之间的强大交互。

在下一节中，我们将探索Gradio如何与MCP集成，为这些能力提供易于使用的接口。 